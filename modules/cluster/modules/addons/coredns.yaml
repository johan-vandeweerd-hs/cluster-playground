replicaCount: 2

resources:
  requests:
    cpu: "250m"
    memory: "256Mi"
  limits:
    memory: "256Mi"

# Use topologySpreadContraints instead
affinity:

tolerations:
  - key: CriticalAddonsOnly
    operator: Exists
  - key: node-role.kubernetes.io/control-plane
    effect: NoSchedule

# When the Karpenter node pool is removed, Karpenter tries to remove all the
# nodes respecting the PDB's. The PDB of Coredns blocks Karpenter from removing
# all nodes which eventually results in Karpenter being undeployed with
# remaining orphan EC2 nodes.
podDisruptionBudget:
  enabled: false

podLabels:
  app: coredns
  kubernetes.io/name: coredns

topologySpreadConstraints:
  - topologyKey: "topology.kubernetes.io/zone"
    maxSkew: 1
    # Set to DoNotSchedule instead of ScheduleAnyway to prevent the
    # kube-scheduler of scheduling coredns on one node when a second node is
    # not ready (yet).
    # See https://karpenter.sh/docs/faq/#why-arent-my-topology-spread-constraints-spreading-pods-across-zones
    whenUnsatisfiable: DoNotSchedule
    minDomains: 3
    labelSelector:
      matchLabels:
        app: coredns
  - topologyKey: "kubernetes.io/hostname"
    maxSkew: 1
    # Set to DoNotSchedule instead of ScheduleAnyway to prevent the
    # kube-scheduler of scheduling coredns on one node when a second node is
    # not ready (yet).
    # See https://karpenter.sh/docs/faq/#why-arent-my-topology-spread-constraints-spreading-pods-across-zones
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: coredns
